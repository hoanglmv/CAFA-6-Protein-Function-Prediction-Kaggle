{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data Viewing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "data_dir = \"../data/Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train_terms.tsv\n",
        "print(\"Loading train_terms.tsv...\")\n",
        "train_terms_path = os.path.join(data_dir, \"train_terms.tsv\")\n",
        "train_terms_df = pd.read_csv(train_terms_path, sep=\"\\t\")\n",
        "print(\"train_terms.tsv head:\")\n",
        "display(train_terms_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "train_sequences_path = os.path.join(data_dir, \"train_sequences.fasta\")\n",
        "\n",
        "sequences = []\n",
        "current_sequence_id = None\n",
        "current_sequence = []\n",
        "current_header = None\n",
        "\n",
        "def parse_header(header_line):\n",
        "    \"\"\"\n",
        "    Input: header_line: string without leading '>'\n",
        "    Return: dict with parsed fields:\n",
        "      - header: full header (raw)\n",
        "      - db_source, accession, entry_name, description, OS, OX, GN, PE, SV\n",
        "    \"\"\"\n",
        "    header = header_line.strip()\n",
        "    result = {'header': header,\n",
        "              'db_source': None, 'accession': None, 'entry_name': None,\n",
        "              'description': None, 'OS': None, 'OX': None, 'GN': None, 'PE': None, 'SV': None}\n",
        "\n",
        "    # primary token (something like sp|A0JNW5|BLT3B_HUMAN)\n",
        "    parts = header.split(' ', 1)\n",
        "    primary = parts[0]\n",
        "    rest = parts[1] if len(parts) > 1 else ''\n",
        "    primary_parts = primary.split('|')\n",
        "    if len(primary_parts) >= 3:\n",
        "        result['db_source'] = primary_parts[0]\n",
        "        result['accession'] = primary_parts[1]\n",
        "        result['entry_name'] = primary_parts[2]\n",
        "    else:\n",
        "        # fallback: set whole primary into entry_name\n",
        "        result['entry_name'] = primary\n",
        "\n",
        "    # description is the text before the first \" OS=\" token (if present)\n",
        "    if ' OS=' in rest:\n",
        "        desc, fields_str = rest.split(' OS=', 1)\n",
        "        result['description'] = desc.strip()\n",
        "        fields_str = 'OS=' + fields_str  # restore the OS= for parsing\n",
        "    else:\n",
        "        result['description'] = rest.strip()\n",
        "        fields_str = ''\n",
        "\n",
        "    # normalize delimiters then split into key=value tokens\n",
        "    if fields_str:\n",
        "        # ensure consistent separators for known keys\n",
        "        for key in [' OX=', ' GN=', ' PE=', ' SV=']:\n",
        "            fields_str = fields_str.replace(key, '|' + key.strip())\n",
        "        # Also prefix OS= if it wasn't prefixed by '|'\n",
        "        fields_str = fields_str.replace('OS=', 'OS=').lstrip('|')\n",
        "        for token in fields_str.split('|'):\n",
        "            if '=' not in token:\n",
        "                continue\n",
        "            k, v = token.split('=', 1)\n",
        "            k = k.strip()\n",
        "            v = v.strip()\n",
        "            if k in result:\n",
        "                result[k] = v\n",
        "            else:\n",
        "                # assign to known ones explicitly\n",
        "                if k == 'OS':\n",
        "                    result['OS'] = v\n",
        "                elif k == 'OX':\n",
        "                    result['OX'] = v\n",
        "                elif k == 'GN':\n",
        "                    result['GN'] = v\n",
        "                elif k == 'PE':\n",
        "                    result['PE'] = v\n",
        "                elif k == 'SV':\n",
        "                    result['SV'] = v\n",
        "    return result\n",
        "\n",
        "# read file safely\n",
        "try:\n",
        "    with open(train_sequences_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip('\\n')\n",
        "            if not line:\n",
        "                continue\n",
        "            if line.startswith('>'):\n",
        "                # save previous sequence\n",
        "                if current_sequence_id is not None:\n",
        "                    seq_str = ''.join(current_sequence)\n",
        "                    row = {\n",
        "                        'sequence_id': current_sequence_id,\n",
        "                        'sequence': seq_str,\n",
        "                        'length': len(seq_str),\n",
        "                    }\n",
        "                    # attach parsed header fields\n",
        "                    row.update(current_header)\n",
        "                    sequences.append(row)\n",
        "\n",
        "                # start new record\n",
        "                header_line = line[1:].strip()\n",
        "                current_header = parse_header(header_line)\n",
        "                current_sequence_id = current_header.get('accession') or current_header.get('entry_name') or header_line\n",
        "                current_sequence = []\n",
        "            else:\n",
        "                # sequence lines: remove whitespace and append\n",
        "                current_sequence.append(line.strip())\n",
        "\n",
        "        # final record\n",
        "        if current_sequence_id is not None:\n",
        "            seq_str = ''.join(current_sequence)\n",
        "            row = {\n",
        "                'sequence_id': current_sequence_id,\n",
        "                'sequence': seq_str,\n",
        "                'length': len(seq_str),\n",
        "            }\n",
        "            row.update(current_header)\n",
        "            sequences.append(row)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"Không tìm thấy file: {train_sequences_path}. Kiểm tra lại data_dir hoặc tên file.\")\n",
        "\n",
        "# build dataframe and display\n",
        "train_sequences_df = pd.DataFrame(sequences)\n",
        "\n",
        "# reorder columns for nicer view (if present)\n",
        "preferred_cols = ['sequence_id', 'accession', 'entry_name', 'db_source', 'description',\n",
        "                  'OS', 'OX', 'GN', 'PE', 'SV', 'length', 'sequence', 'header'] \n",
        "cols = [c for c in preferred_cols if c in train_sequences_df.columns] + \\\n",
        "       [c for c in train_sequences_df.columns if c not in preferred_cols]\n",
        "\n",
        "train_sequences_df = train_sequences_df[cols]\n",
        "\n",
        "print(\"train_sequences.fasta head:\")\n",
        "display(train_sequences_df.head())\n",
        "print(\"\\nTotal sequences loaded:\", len(train_sequences_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train_taxonomy.tsv\n",
        "print(\"Loading train_taxonomy.tsv...\")\n",
        "train_taxonomy_path = os.path.join(data_dir, \"train_taxonomy.tsv\")\n",
        "train_taxonomy_df = pd.read_csv(train_taxonomy_path, sep=\"\\t\")\n",
        "print(\"train_taxonomy.tsv head:\")\n",
        "display(train_taxonomy_df.head())\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load go-basic.obo into a DataFrame\n",
        "print(\"Loading go-basic.obo...\")\n",
        "go_obo_path = os.path.join(data_dir, \"go-basic.obo\")\n",
        "terms = []\n",
        "current_term = {}\n",
        "\n",
        "with open(go_obo_path, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line == '[Term]':\n",
        "            if current_term:\n",
        "                terms.append(current_term)\n",
        "            current_term = {}\n",
        "        elif line.startswith('id:'):\n",
        "            current_term['id'] = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('name:'):\n",
        "            current_term['name'] = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('namespace:'):\n",
        "            current_term['namespace'] = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('def:'):\n",
        "            current_term['def'] = line.split(':', 1)[1].strip().strip('\"')\n",
        "        elif line.startswith('is_a:'):\n",
        "            # Extract only the GO ID, ignore the name after '!'\n",
        "            is_a_id = line.split(':', 1)[1].split('!', 1)[0].strip()\n",
        "            if 'is_a' not in current_term:\n",
        "                current_term['is_a'] = []\n",
        "            current_term['is_a'].append(is_a_id)\n",
        "    if current_term:  # Add the last term\n",
        "        terms.append(current_term)\n",
        "\n",
        "go_obo_df = pd.DataFrame(terms)\n",
        "print(\"go-basic.obo head:\")\n",
        "display(go_obo_df.head())\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data loading complete.\")\n",
        "print(\"train_sequences_df columns:\", train_sequences_df.columns)\n",
        "print(\"train_taxonomy_df columns:\", train_taxonomy_df.columns)\n",
        "print(\"go_obo_df columns:\", go_obo_df.columns)\n",
        "print(\"train_terms_df columns:\", train_terms_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, warnings, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, average_precision_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF limits to avoid huge dense tensors on GPU\n",
        "TFIDF_MAX_FEATURES = 50000   # lower if memory is tight\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 8\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "DROPOUT = 0.3\n",
        "HIDDEN_DIM = 1024\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- 0. Load precomputed resources (assume train_terms_df and train_sequences_df exist) ----------\n",
        "# If not present, load from files (adapt paths as needed)\n",
        "if 'train_terms_df' not in globals():\n",
        "    train_terms_df = pd.read_csv(os.path.join(TRAIN_DIR, 'train_terms.tsv'), sep='\\t')\n",
        "if 'train_sequences_df' not in globals():\n",
        "    # use your existing parse function or data already loaded in session\n",
        "    raise RuntimeError(\"train_sequences_df not found in globals. Load it first.\")\n",
        "\n",
        "# map sequences: sequence_id -> sequence\n",
        "seq_map = dict(zip(train_sequences_df['sequence_id'], train_sequences_df['sequence']))\n",
        "all_entry_ids = set(train_terms_df['EntryID'].unique()).intersection(seq_map.keys())\n",
        "entries = sorted(all_entry_ids)\n",
        "seqs = [seq_map[e] for e in entries]\n",
        "print(f\"Train proteins with sequences: {len(entries)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- 1. TF-IDF features ----------\n",
        "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3,3), min_df=2, max_features=TFIDF_MAX_FEATURES)\n",
        "X_sparse = vectorizer.fit_transform(seqs)\n",
        "print(\"TF-IDF shape (sparse):\", X_sparse.shape)\n",
        "\n",
        "# ---------- 2. Build multi-label Y (we'll train per-aspect; here show MF/CC/BP loop as before) ----------\n",
        "aspect_names = {'F': 'MF', 'P': 'BP', 'C': 'CC'}\n",
        "models_summary = {}\n",
        "def eval_best_threshold(y_true, y_prob, thresholds=np.linspace(0.1,0.9,17)):\n",
        "    best_f1, best_t = 0.0, 0.5\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    return best_t, best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- 3. loop aspects and train PyTorch MLP ----------\n",
        "for aspect_code, aspect_label in aspect_names.items():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Training aspect {aspect_label} ({aspect_code})\")\n",
        "    # gather labels for proteins in 'entries' order\n",
        "    labels_map = train_terms_df[train_terms_df['aspect']==aspect_code].groupby('EntryID')['term'].apply(list).to_dict()\n",
        "    y_labels = [labels_map.get(e, []) for e in entries]\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    Y = mlb.fit_transform(y_labels)\n",
        "    n_labels = Y.shape[1]\n",
        "    print(\"Number of labels (terms):\", n_labels)\n",
        "    if n_labels == 0:\n",
        "        print(\"No labels for this aspect; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # train/val split\n",
        "    X_train_idx, X_val_idx, Y_train, Y_val = train_test_split(\n",
        "        np.arange(len(entries)), Y, test_size=0.2, random_state=RANDOM_STATE)\n",
        "    # convert sparse -> dense float32 (be careful with memory). If too big, reduce TFIDF_MAX_FEATURES.\n",
        "    X = X_sparse.tocsr()\n",
        "    X_train = X[X_train_idx].toarray().astype(np.float32)\n",
        "    X_val = X[X_val_idx].toarray().astype(np.float32)\n",
        "    print(\"Dense shapes:\", X_train.shape, X_val.shape)\n",
        "\n",
        "    # Build datasets and dataloaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train.astype(np.float32)))\n",
        "    val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(Y_val.astype(np.float32)))\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Simple MLP\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim, out_dim, dropout=0.3):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, hidden_dim//2),\n",
        "                nn.BatchNorm1d(hidden_dim//2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim//2, out_dim)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    model = MLP(input_dim=X_train.shape[1], hidden_dim=HIDDEN_DIM, out_dim=n_labels, dropout=DROPOUT).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # training loop with tqdm\n",
        "    best_val_f1 = -1.0\n",
        "    best_state = None\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} train\", leave=False)\n",
        "        for xb, yb in pbar:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_probs = []\n",
        "        all_trues = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device); yb = yb.to(device)\n",
        "                logits = model(xb)\n",
        "                loss = criterion(logits, yb)\n",
        "                val_loss += loss.item() * xb.size(0)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "                all_trues.append(yb.cpu().numpy())\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        all_probs = np.vstack(all_probs)\n",
        "        all_trues = np.vstack(all_trues)\n",
        "\n",
        "        # compute micro F1 at default threshold 0.5 and also best threshold\n",
        "        f1_05 = f1_score(all_trues, (all_probs>=0.5).astype(int), average='micro', zero_division=0)\n",
        "        best_t, best_f1 = eval_best_threshold(all_trues, all_probs, thresholds=np.linspace(0.1,0.9,17))\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | train_loss={epoch_loss:.4f} val_loss={val_loss:.4f} val_f1@0.5={f1_05:.4f} best_f1={best_f1:.4f} (t={best_t:.2f})\")\n",
        "\n",
        "        # save best\n",
        "        if best_f1 > best_val_f1:\n",
        "            best_val_f1 = best_f1\n",
        "            best_state = {\n",
        "                'model_state': model.state_dict(),\n",
        "                'mlb_classes': list(mlb.classes_),\n",
        "                'vectorizer_vocab_size': X_train.shape[1],\n",
        "                'best_threshold': float(best_t),\n",
        "                'val_f1': float(best_f1)\n",
        "            }\n",
        "\n",
        "    # after epochs: restore best state and save model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state['model_state'])\n",
        "        # Save model checkpoint (PyTorch)\n",
        "        save_dir = os.path.join(BASE_DIR, 'models')\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        fname = f\"mlp_aspect_{aspect_label}.pth\"\n",
        "        torch.save({\n",
        "            'model_state': model.state_dict(),\n",
        "            'mlb_classes': best_state['mlb_classes'],\n",
        "            'vectorizer': vectorizer,   # caution: may be large; you can save separately via pickle\n",
        "            'best_threshold': best_state['best_threshold'],\n",
        "            'val_f1': best_state['val_f1']\n",
        "        }, os.path.join(save_dir, fname))\n",
        "        print(f\"Saved best model for {aspect_label} to {os.path.join(save_dir, fname)} (val_f1={best_state['val_f1']:.4f})\")\n",
        "\n",
        "    models_summary[aspect_code] = {\n",
        "        'n_labels': n_labels,\n",
        "        'best_val_f1': float(best_val_f1),\n",
        "        'best_threshold': float(best_state['best_threshold']) if best_state is not None else None,\n",
        "        'model_path': os.path.join(save_dir, fname) if best_state is not None else None\n",
        "    }\n",
        "\n",
        "print(\"\\nTraining summary:\")\n",
        "print(json.dumps(models_summary, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
