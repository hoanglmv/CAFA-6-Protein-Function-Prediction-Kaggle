import numpy as np
from typing import List
from sentence_transformers import SentenceTransformer
import torch

# Global variable to store the model instance (Singleton pattern)
_model = None

def embed_labels(
    texts: List[str], 
    batch_size: int = 32, # Default n√™n > 1 ƒë·ªÉ t·∫≠n d·ª•ng GPU
    device: str = "cpu",  # <--- B·∫ÆT BU·ªòC C√ì ƒë·ªÉ nh·∫≠n tham s·ªë t·ª´ label_processing.py
    show_progress_bar: bool = True
) -> np.ndarray:
    """
    Embed a list of text descriptions using PubMedBERT via SentenceTransformer.

    Args:
        texts: List of text strings to embed.
        batch_size: Batch size for processing.
        device: Device to run the model on ('cpu', 'cuda', etc.).
        show_progress_bar: Whether to show a progress bar.

    Returns:
        embeddings: Numpy array of embeddings with shape (len(texts), 768).
    """
    global _model
    
    # T√™n model b·∫°n mu·ªën d√πng
    model_name = "NeuML/pubmedbert-base-embeddings"

    if _model is None:
        print(f"üì• Loading SentenceTransformer model: {model_name}")
        print(f"‚öôÔ∏è  Device: {device}")
        # Kh·ªüi t·∫°o model v√† ƒë∆∞a v√†o GPU
        _model = SentenceTransformer(model_name, device=device)

    # SentenceTransformer t·ª± ƒë·ªông x·ª≠ l√Ω tokenization v√† batching
    embeddings = _model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=show_progress_bar,
        convert_to_numpy=True,
        device=device,            # ƒê·∫£m b·∫£o ch·∫°y tr√™n ƒë√∫ng device
        normalize_embeddings=True # T·ªët cho c√°c t√°c v·ª• similarity
    )

    return embeddings