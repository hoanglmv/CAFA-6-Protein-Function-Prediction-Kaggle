import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import pandas as pd
import numpy as np
from tqdm import tqdm
import pickle

# Add src to path to import model
# Assuming script is in src/tungvmh_ver2/
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

try:
    from src.encode.loss_function import AsymmetricLoss
    from src.align_embed.protein_go_aligner import ProteinGOAligner
except ImportError:
    # Fallback if specific paths are different, try to find them
    # For now assume they exist as in previous version
    pass

# Configuration
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
PROCESSED_DIR = os.path.join(DATA_DIR, "processed_ver2")
PROCESSED2_DIR = os.path.join(DATA_DIR, "processed2_ver2")

TRAIN_PATH = os.path.join(PROCESSED2_DIR, "train_complete.parquet")
VOCAB_PATH = os.path.join(PROCESSED_DIR, "vocab.pkl")

MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, "models_ver2")
MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, "align_model.pth")

BATCH_SIZE = 1024
EPOCHS = 50 # Adjusted for faster iteration, user can increase
LEARNING_RATE = 3e-4
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class ProteinGODataset(Dataset):
    def __init__(self, train_df, num_classes):
        self.train_df = train_df
        self.num_classes = num_classes

        print("Pre-loading training embeddings to RAM...")
        self.embeddings = torch.tensor(
            np.stack(train_df["embedding"].values), dtype=torch.float32
        )
        # go_terms_id are already indices 0-4999
        self.go_terms_id_list = train_df["go_terms_id"].values

    def __len__(self):
        return len(self.train_df)

    def __getitem__(self, idx):
        prot_emb = self.embeddings[idx]
        label_vec = torch.zeros(self.num_classes, dtype=torch.float32)

        indices = self.go_terms_id_list[idx]
        if indices is not None and len(indices) > 0:
            label_vec[indices] = 1.0

        return prot_emb, label_vec


def train():
    print(f"Using device: {DEVICE}")
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

    # 1. Load Vocab
    print(f"Loading vocab from {VOCAB_PATH}...")
    with open(VOCAB_PATH, "rb") as f:
        vocab_data = pickle.load(f)
    
    top_terms = vocab_data["top_terms"] # List of 5000 terms
    term_to_idx = vocab_data["term_to_idx"]
    num_classes = len(top_terms)
    print(f"Vocab size: {num_classes}")

    # 2. Load Label Embeddings (Generated by label_processing.py)
    LABEL_PATH = os.path.join(PROCESSED2_DIR, "label.parquet")
    print(f"Loading label embeddings from {LABEL_PATH}...")
    
    if not os.path.exists(LABEL_PATH):
        print(f"Error: Label embedding file not found at {LABEL_PATH}")
        print("Please run src/tungvmh_ver2/label_processing.py first.")
        return

    label_df = pd.read_parquet(LABEL_PATH)
    # label_df has columns: id (0-4999), name (GO ID), embedding
    
    # Sort by ID to ensure correct order matching the vocab indices
    label_df = label_df.sort_values("id")
    
    # Check if we have all 5000 terms
    if len(label_df) != num_classes:
        print(f"Warning: Label file has {len(label_df)} terms, expected {num_classes}.")
    
    train_go_embeddings = np.stack(label_df["embedding"].values)
    
    go_embeddings_tensor = torch.tensor(
        train_go_embeddings, dtype=torch.float32
    ).to(DEVICE)
    print(f"GO Embeddings Shape: {go_embeddings_tensor.shape}")

    # 3. Load Training Data
    print(f"Loading training data from {TRAIN_PATH}...")
    if not os.path.exists(TRAIN_PATH):
        print("Train data not found. Please run data processing first.")
        return

    train_df = pd.read_parquet(TRAIN_PATH)
    print(f"Total samples: {len(train_df)}")

    # Dataset
    full_dataset = ProteinGODataset(train_df, num_classes=num_classes)

    # Split
    train_size = int(0.99 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    # Model
    model = ProteinGOAligner(esm_dim=2560, go_emb_dim=768).to(DEVICE)

    # Loss & Optimizer
    criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # Scheduler
    warmup_epochs = 2
    warmup_scheduler = optim.lr_scheduler.LinearLR(
        optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs
    )
    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=(EPOCHS - warmup_epochs), eta_min=1e-6
    )
    scheduler = optim.lr_scheduler.SequentialLR(
        optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs]
    )

    # Training Loop
    best_val_loss = float("inf")
    patience = 10
    patience_counter = 0

    print("Starting training...")
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for prot_emb, labels in pbar:
            prot_emb = prot_emb.to(DEVICE, non_blocking=True)
            labels = labels.to(DEVICE, non_blocking=True)

            optimizer.zero_grad()
            logits = model(prot_emb, go_embeddings_tensor)
            loss = criterion(logits, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_loss += loss.item()
            pbar.set_postfix({"loss": loss.item()})

        avg_train_loss = train_loss / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for prot_emb, labels in val_loader:
                prot_emb = prot_emb.to(DEVICE)
                labels = labels.to(DEVICE)
                logits = model(prot_emb, go_embeddings_tensor)
                val_loss += criterion(logits, labels).item()

        avg_val_loss = val_loss / len(val_loader)
        
        scheduler.step()
        current_lr = optimizer.param_groups[0]["lr"]

        print(f"Epoch {epoch+1} | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f} | LR: {current_lr:.2e}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            print("Saved Best Model")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping.")
                break

if __name__ == "__main__":
    train()
